# Proyecto ETL â€“ AnÃ¡lisis de Ingresos con Ajuste por InflaciÃ³n (Argentina)

## ğŸ“Œ DescripciÃ³n general

Este proyecto implementa un **pipeline ETL completo** orientado al **anÃ¡lisis econÃ³mico de los ingresos** de un negocio en el contexto argentino, caracterizado por **procesos inflacionarios sostenidos**.

El principal objetivo es **analizar los ingresos reales del negocio**, considerando:

* su **evoluciÃ³n a lo largo del tiempo**, ajustada por inflaciÃ³n mediante la **cotizaciÃ³n histÃ³rica del dÃ³lar**
* la **composiciÃ³n de los ingresos** segÃºn medio de pago y condiciÃ³n fiscal
* el **impacto de los componentes impositivos** sobre los montos brutos y netos

De esta forma, el proyecto permite evaluar no solo cuÃ¡nto se factura, sino **cÃ³mo se compone ese ingreso, cÃ³mo evoluciona en tÃ©rminos reales y quÃ© proporciÃ³n se destina al pago de impuestos**.

El proyecto fue desarrollado en el marco de un **bootcamp de Data Analytics**, con un enfoque deliberadamente mÃ¡s cercano a un **entorno profesional / productivo**.


## ğŸ¯ Problema de negocio

Los datos de ventas se recolectaron a lo largo de varios aÃ±os mediante carga manual en Google Sheets. Esto genera mÃºltiples desafÃ­os:

* Valores nominales no comparables entre aÃ±os debido a la inflaciÃ³n
* Montos escritos en texto libre (mezcla de nÃºmeros, monedas y comentarios)
* Registro de ventas tanto en ARS como en USD
* Necesidad de distinguir ingresos fiscales y no fiscales

Analizar estos datos sin un proceso de limpieza y estandarizaciÃ³n conduce a conclusiones distorsionadas.

---

## ğŸ’¡ SoluciÃ³n propuesta

Se diseÃ±Ã³ un **pipeline ETL modular** que:

1. Extrae datos de ventas desde Google Sheets
2. Extrae la cotizaciÃ³n histÃ³rica del dÃ³lar desde una API externa
3. Limpia y normaliza montos monetarios (incluyendo texto libre)
4. Convierte valores a una base comparable en el tiempo
5. Calcula impuestos segÃºn reglas de negocio
6. Modela los datos en un **esquema tipo estrella (Star Schema)**
7. Carga la informaciÃ³n en PostgreSQL para su anÃ¡lisis posterior en herramientas BI

---

## ğŸ§± Arquitectura del proyecto

```
Google Sheets + API DÃ³lar
        â†“
     EXTRACT
        â†“
    TRANSFORM
        â†“
PostgreSQL (Docker)
        â†“
   Power BI
```

---

## ğŸ—‚ï¸ Estructura del proyecto

```
.
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ extraction.py      # ExtracciÃ³n de datos (Sheets + API DÃ³lar)
â”‚   â”œâ”€â”€ transformation.py  # Limpieza, normalizaciÃ³n y reglas de negocio
â”‚   â”œâ”€â”€ loading.py         # Carga a PostgreSQL
â”‚   â”œâ”€â”€ config.py          # ConfiguraciÃ³n general
â”‚   â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ main.py                # Orquestador del pipeline ETL
â”œâ”€â”€ docker-compose.yml     # PostgreSQL (Docker)
â”œâ”€â”€ requirements.txt       # Dependencias del proyecto
â”œâ”€â”€ pyproject.toml         # Metadata del proyecto
â”œâ”€â”€ README.md
â””â”€â”€ .gitignore
```

---

## ğŸ—ƒï¸ Modelo de datos

El modelo sigue un enfoque **dimensional (Star Schema)**:

### ğŸ“Š Tabla de hechos

* **fact_ingresos**: registra los ingresos normalizados y auditados

### ğŸ“ Dimensiones

* **dim_calendario**: anÃ¡lisis temporal
* **dim_medios_pago**: clasificaciÃ³n por medio de cobro y fiscalidad
* **dim_cotizacion**: cotizaciÃ³n histÃ³rica del dÃ³lar

Este modelo facilita el anÃ¡lisis posterior en herramientas de BI.

---

## ğŸ› ï¸ TecnologÃ­as utilizadas

* **Python 3.10+**
* **Pandas / NumPy**
* **Google Sheets API (gspread)**
* **Requests (API externa)**
* **PostgreSQL**
* **SQLAlchemy**
* **Docker & Docker Compose**
* **Power BI**

---

## ğŸš€ CÃ³mo levantar el proyecto

### 1ï¸âƒ£ Clonar el repositorio

```bash
git clone <url-del-repositorio>
cd proyecto-bootcamp-devlights
```

---

### 2ï¸âƒ£ Crear y activar el entorno virtual (uv)

```bash
uv venv
uv pip install -r requirements.txt
```

---

### 3ï¸âƒ£ ConfiguraciÃ³n de acceso a Google Sheets

El pipeline consume datos desde Google Sheets mediante la API oficial de Google.

Para ejecutar el proyecto en un entorno local es necesario contar con credenciales vÃ¡lidas
(este repositorio **no incluye credenciales reales**, ya que se utilizan datos de un cliente).

De forma general, el proceso consiste en:

* Crear un **Service Account** en Google Cloud
* Generar un archivo de credenciales (`credentials.json`)
* Definir la ubicaciÃ³n del archivo mediante variables de entorno o configuraciÃ³n local
* Compartir el Google Sheet con el email del Service Account

> âš ï¸ Nota: las credenciales no deben versionarse ni subirse al repositorio.
En un entorno productivo, estas credenciales deberÃ­an gestionarse mediante un
servicio de secretos o variables de entorno seguras.
---

### 4ï¸âƒ£ Levantar la infraestructura con Docker

```bash
docker-compose up -d
```

Servicios disponibles:

* PostgreSQL â†’ `localhost:5440`

---

### 5ï¸âƒ£ Ejecutar el pipeline ETL

```bash
python main.py
```

Al finalizar, las tablas quedarÃ¡n cargadas en PostgreSQL.

---

## ğŸ“ˆ AnÃ¡lisis y visualizaciÃ³n

Los datos cargados en PostgreSQL se analizan utilizando **Power BI**, conectÃ¡ndose directamente a la base de datos.

El modelo dimensional permite:

* AnÃ¡lisis temporal
* ComparaciÃ³n real de ingresos ajustados por inflaciÃ³n
* SegmentaciÃ³n por medio de pago
* AnÃ¡lisis de impuestos, montos netos y rentabilidad

---

## ğŸ§  Consideraciones analÃ­ticas

* La cotizaciÃ³n histÃ³rica del dÃ³lar se utiliza como **referencia temporal** para mitigar distorsiones inflacionarias
* La limpieza de texto libre incluye **etiquetas de auditorÃ­a** para trazabilidad
* El pipeline estÃ¡ diseÃ±ado para ser **escalable y automatizable**

---

## ğŸ‘¤ Autor

**Alan Prestia**
Data Analyst / Instructor de ProgramaciÃ³n
ğŸ“§ [alaneprestia@gmail.com](mailto:alaneprestia@gmail.com)

---

## âš ï¸ Nota

Este proyecto fue desarrollado con fines educativos en el marco de un bootcamp, utilizando **datos reales de un cliente**, tratados con criterios de confidencialidad y anonimizaciÃ³n. 
