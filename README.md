# Proyecto ETL â€“ AnÃ¡lisis de Ingresos con Ajuste por InflaciÃ³n (Argentina)

## ğŸ“Œ DescripciÃ³n general

Este proyecto implementa un **pipeline ETL completo** orientado al **anÃ¡lisis econÃ³mico y fiscal de los ingresos** de un negocio en el contexto argentino, caracterizado por **procesos inflacionarios sostenidos**.

El principal objetivo es **analizar los ingresos reales del negocio**, considerando:

* su **evoluciÃ³n a lo largo del tiempo**, ajustada por inflaciÃ³n mediante la **cotizaciÃ³n histÃ³rica del dÃ³lar**
* la **composiciÃ³n de los ingresos** segÃºn medio de pago y condiciÃ³n fiscal
* la **proporcion de ingresos registrados y no registrados**
* el **impacto de los componentes impositivos** sobre los montos brutos y netos

De esta forma, el proyecto permite evaluar no solo cuÃ¡nto se factura, sino **cÃ³mo se compone ese ingreso, cÃ³mo evoluciona en tÃ©rminos reales y quÃ© proporciÃ³n se destina al pago de impuestos**.

El proyecto fue desarrollado en el marco de un **bootcamp de Data Analytics**, con un enfoque deliberadamente mÃ¡s cercano a un **entorno profesional / productivo**.


## ğŸ¯ Problema de negocio

Los datos de ventas se recolectaron a lo largo de varios aÃ±os mediante carga manual en Google Sheets. Esto genera mÃºltiples desafÃ­os:

* Valores nominales no comparables entre aÃ±os debido a la inflaciÃ³n
* Montos escritos en texto libre (mezcla de nÃºmeros, monedas y comentarios)
* Registro de ventas tanto en ARS como en USD
* Necesidad de distinguir ingresos fiscales y no fiscales, asÃ­ como su impacto impositivo

Analizar estos datos sin un proceso de limpieza, estandarizaciÃ³n y auditorÃ­a conduce a conclusiones incompletas o distorsionadas, especialmente en contextos de alta inflaciÃ³n.

---

## ğŸ’¡ SoluciÃ³n propuesta

Se diseÃ±Ã³ un **pipeline ETL modular** que:

1. Extrae datos de ventas desde Google Sheets
2. Extrae la cotizaciÃ³n histÃ³rica del dÃ³lar desde una API externa
3. Limpia y normaliza montos monetarios (incluyendo texto libre)
4. Convierte valores a una base comparable en el tiempo
5. Calcula impuestos segÃºn reglas de negocio
6. Modela los datos en un **esquema tipo estrella (Star Schema)**
7. Carga la informaciÃ³n en PostgreSQL para su anÃ¡lisis posterior en herramientas BI

---

## ğŸ§± Arquitectura del proyecto

```
Google Sheets + API DÃ³lar
        â†“
     EXTRACT
        â†“
    TRANSFORM
        â†“
PostgreSQL (Docker)
        â†“
   Power BI
```

---

## ğŸ—‚ï¸ Estructura del proyecto

```
.
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ extraction.py      # ExtracciÃ³n de datos (Sheets + API DÃ³lar)
â”‚   â”œâ”€â”€ transformation.py  # Limpieza, normalizaciÃ³n y reglas de negocio
â”‚   â”œâ”€â”€ loading.py         # Carga a PostgreSQL
â”‚   â”œâ”€â”€ config.py          # ConfiguraciÃ³n general
â”‚   â””â”€â”€ __init__.py
â”‚
â”‚â”€â”€ docker/
â”‚   â””â”€â”€ docker-compose.yml # PostgreSQL (Docker)
â”‚â”€â”€ reports/
â”‚   â””â”€â”€ 
â”œâ”€â”€ main.py                # Orquestador del pipeline ETL 
â”œâ”€â”€ requirements.txt       # Dependencias del proyecto
â”œâ”€â”€ pyproject.toml         # Metadata del proyecto
â”œâ”€â”€ README.md
â””â”€â”€ .gitignore
```

---

## ğŸ—ƒï¸ Modelo de datos

El modelo sigue un enfoque **dimensional (Star Schema)**:

### ğŸ“Š Tabla de hechos

* **fact_ingresos**: registra los ingresos normalizados y auditados

### ğŸ“ Dimensiones

* **dim_calendario**: anÃ¡lisis temporal
* **dim_medios_pago**: clasificaciÃ³n por medio de cobro y fiscalidad
* **dim_cotizacion**: cotizaciÃ³n histÃ³rica del dÃ³lar

Este modelo facilita el anÃ¡lisis posterior en herramientas de BI.

---

## ğŸ› ï¸ TecnologÃ­as utilizadas

* **Python 3.10+**
* **Pandas / NumPy**
* **Google Sheets API (gspread)**
* **Requests (API externa)**
* **PostgreSQL**
* **SQLAlchemy**
* **Docker & Docker Compose**
* **Power BI**

---

## ğŸš€ CÃ³mo levantar el proyecto

### 1ï¸âƒ£ Clonar el repositorio

```bash
git clone <https://github.com/AlanEPrestia/etl-analisis-ingresos-impuestos-ajuste-inflacion.git>
cd proyecto-bootcamp-devlights
```

---

### 2ï¸âƒ£ Crear y activar el entorno virtual (uv)

```bash
uv venv
uv pip install -r requirements.txt
```

---

### 3ï¸âƒ£ ConfiguraciÃ³n de acceso a Google Sheets

El pipeline obtiene los datos desde **Google Sheets** mediante la API oficial de Google,
utilizando un **Service Account**.

Para ejecutar el proyecto en un entorno local es obligatorio contar con credenciales vÃ¡lidas.
Este repositorio **no incluye credenciales reales**, ya que se trabaja con datos de un cliente
y con informaciÃ³n sensible.

#### Tipo de credenciales requeridas

El proyecto espera un archivo de credenciales con las siguientes caracterÃ­sticas:

- **Tipo:** Google Cloud Service Account  
- **Formato:** JSON  
- **Permisos mÃ­nimos:** acceso de lectura al Google Sheet  
- **Archivo:** `credentials.json` (no versionado)

El Google Sheet utilizado como fuente de datos debe estar compartido con el email
asociado al Service Account.

#### ConfiguraciÃ³n local

La ubicaciÃ³n del archivo de credenciales debe definirse mediante una variable de entorno:

```bash
GOOGLE_APPLICATION_CREDENTIALS=/ruta/absoluta/credentials.json



### 4ï¸âƒ£ Levantar la infraestructura con Docker

```bash
docker-compose up -d
```

Servicios disponibles:

* PostgreSQL â†’ `localhost:5440`

---

### 5ï¸âƒ£ Ejecutar el pipeline ETL

```bash
python main.py
```

Al finalizar, las tablas quedarÃ¡n cargadas en PostgreSQL.

---

## ğŸ“ˆ AnÃ¡lisis y visualizaciÃ³n

Los datos cargados en PostgreSQL se analizan utilizando **Power BI**, conectÃ¡ndose directamente a la base de datos.

El modelo dimensional permite:

* AnÃ¡lisis temporal
* ComparaciÃ³n real de ingresos ajustados por inflaciÃ³n
* SegmentaciÃ³n por medio de pago
* AnÃ¡lisis de ingresos registrados vs no registrados
* AnÃ¡lisis de impuestos, montos netos y rentabilidad

---

## ğŸ§  Consideraciones analÃ­ticas

* La cotizaciÃ³n histÃ³rica del dÃ³lar se utiliza como **referencia temporal** para mitigar distorsiones inflacionarias
* La limpieza de texto libre incluye **etiquetas de auditorÃ­a** para trazabilidad
* El pipeline estÃ¡ diseÃ±ado para ser **escalable y automatizable**

---

## ğŸ‘¤ Autor

**Alan Prestia**
Data Analyst / Instructor de ProgramaciÃ³n
ğŸ“§ [alaneprestia@gmail.com](mailto:alaneprestia@gmail.com)

---

## âš ï¸ Nota

Este proyecto fue desarrollado con fines educativos en el marco de un bootcamp, utilizando **datos reales de un cliente**, tratados con criterios de confidencialidad y anonimizaciÃ³n. 

## Contexto del proyecto

Este proyecto fue desarrollado siguiendo **requerimientos formales de un cliente**,
trabajando con **datos reales de operaciÃ³n**.  
Por este motivo, el repositorio no incluye informaciÃ³n sensible ni credenciales reales,
y ciertos aspectos del sistema fueron diseÃ±ados priorizando confidencialidad,
trazabilidad y consistencia de los datos.

El objetivo principal del pipeline es **construir una base analÃ­tica confiable**
a partir de fuentes no estructuradas, aplicando criterios de limpieza,
auditorÃ­a y modelado orientado a Business Intelligence.

## Alcance y supuestos actuales

- El pipeline se ejecuta en modo **batch**.
- La ingesta de datos es **completa** en cada ejecuciÃ³n.
- No se implementa aÃºn una tabla de auditorÃ­a persistente del proceso.
- Las transformaciones responden a reglas de negocio vigentes al momento
  del desarrollo del proyecto.

## PrÃ³ximos pasos (Roadmap)

Como evoluciÃ³n natural del sistema, se consideran los siguientes pasos:

- ImplementaciÃ³n de **ingesta incremental** basada en marcas temporales.
- IncorporaciÃ³n de una **tabla de auditorÃ­a** para registrar ejecuciones,
  volÃºmenes procesados y eventos relevantes.
- OptimizaciÃ³n del proceso de carga para escenarios de mayor volumen.




